{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import time\n",
        "import copy\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements the positional encoding as described in the paper.\n",
        "    Uses sine and cosine functions of different frequencies.\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, max_seq_length=5000, dropout=0.1):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        # Create positional encoding matrix\n",
        "        pe = torch.zeros(max_seq_length, d_model)\n",
        "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "\n",
        "        # Apply sine to even indices\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "\n",
        "        # Apply cosine to odd indices\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        # Add batch dimension [1, max_seq_length, d_model]\n",
        "        pe = pe.unsqueeze(0)\n",
        "\n",
        "        # Register buffer (persistent state)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Tensor, shape [batch_size, seq_len, embedding_dim]\n",
        "        \"\"\"\n",
        "        x = x + self.pe[:, :x.size(1), :]\n",
        "        return self.dropout(x)\n",
        "\n",
        "\n",
        "\n",
        "def scaled_dot_product_attention(query, key, value, mask=None, dropout=None):\n",
        "    \"\"\"\n",
        "    Computes scaled dot product attention as described in the paper.\n",
        "\n",
        "    Args:\n",
        "        query: Tensor, shape [..., seq_len_q, depth]\n",
        "        key: Tensor, shape [..., seq_len_k, depth]\n",
        "        value: Tensor, shape [..., seq_len_k, depth_v]\n",
        "        mask: Tensor that masks out certain positions, shape [..., seq_len_q, seq_len_k]\n",
        "        dropout: Dropout layer\n",
        "\n",
        "    Returns:\n",
        "        output: Weighted sum\n",
        "        attention_weights: Attention weights\n",
        "    \"\"\"\n",
        "    # Calculate QÂ·K^T\n",
        "    matmul_qk = torch.matmul(query, key.transpose(-2, -1))\n",
        "\n",
        "    # Scale matmul_qk\n",
        "    d_k = query.size(-1)\n",
        "    scaled_attention_logits = matmul_qk / math.sqrt(d_k)\n",
        "\n",
        "    # Apply mask if provided\n",
        "    if mask is not None:\n",
        "        scaled_attention_logits = scaled_attention_logits.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "    # Apply softmax to get attention weights\n",
        "    attention_weights = F.softmax(scaled_attention_logits, dim=-1)\n",
        "\n",
        "    # Apply dropout if provided\n",
        "    if dropout is not None:\n",
        "        attention_weights = dropout(attention_weights)\n",
        "\n",
        "    # Calculate weighted sum\n",
        "    output = torch.matmul(attention_weights, value)\n",
        "\n",
        "    return output, attention_weights\n",
        "\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Multi-head attention as described in the paper.\n",
        "    Allows the model to jointly attend to information from different positions.\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.depth = d_model // num_heads\n",
        "\n",
        "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "\n",
        "        # Linear projections\n",
        "        self.wq = nn.Linear(d_model, d_model)\n",
        "        self.wk = nn.Linear(d_model, d_model)\n",
        "        self.wv = nn.Linear(d_model, d_model)\n",
        "\n",
        "        # Output projection\n",
        "        self.wo = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    def split_heads(self, x, batch_size):\n",
        "        \"\"\"\n",
        "        Split the last dimension into (num_heads, depth)\n",
        "        and transpose the result to (batch_size, num_heads, seq_len, depth)\n",
        "        \"\"\"\n",
        "        x = x.view(batch_size, -1, self.num_heads, self.depth)\n",
        "        return x.permute(0, 2, 1, 3)\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        batch_size = query.size(0)\n",
        "\n",
        "        # Linear projections and split heads\n",
        "        query = self.split_heads(self.wq(query), batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
        "        key = self.split_heads(self.wk(key), batch_size)      # (batch_size, num_heads, seq_len_k, depth)\n",
        "        value = self.split_heads(self.wv(value), batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
        "\n",
        "        # Scaled dot-product attention\n",
        "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
        "            query, key, value, mask, self.dropout\n",
        "        )\n",
        "\n",
        "        # Reshape back to (batch_size, seq_len_q, d_model)\n",
        "        scaled_attention = scaled_attention.permute(0, 2, 1, 3).contiguous()\n",
        "        concat_attention = scaled_attention.view(batch_size, -1, self.d_model)\n",
        "\n",
        "        # Final projection\n",
        "        output = self.wo(concat_attention)\n",
        "\n",
        "        return output, attention_weights\n",
        "\n",
        "\n",
        "\n",
        "class PositionwiseFeedForward(nn.Module):\n",
        "    \"\"\"\n",
        "    Simple feed-forward network applied to each position separately and identically.\n",
        "    Consists of two linear transformations with a ReLU activation in between.\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        super(PositionwiseFeedForward, self).__init__()\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Tensor, shape [batch_size, seq_len, d_model]\n",
        "        \"\"\"\n",
        "        x = F.relu(self.linear1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.linear2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "class LayerNormalization(nn.Module):\n",
        "    \"\"\"\n",
        "    Layer normalization module.\n",
        "    Normalizes the inputs across feature dimension.\n",
        "    \"\"\"\n",
        "    def __init__(self, features, eps=1e-6):\n",
        "        super(LayerNormalization, self).__init__()\n",
        "        self.gamma = nn.Parameter(torch.ones(features))\n",
        "        self.beta = nn.Parameter(torch.zeros(features))\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(-1, keepdim=True)\n",
        "        std = x.std(-1, keepdim=True)\n",
        "        return self.gamma * (x - mean) / (std + self.eps) + self.beta\n",
        "\n",
        "\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Single layer of the encoder.\n",
        "    Consists of multi-head self-attention and position-wise feed-forward networks.\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "\n",
        "        # Multi-head attention\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
        "\n",
        "        # Feed-forward network\n",
        "        self.feed_forward = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
        "\n",
        "        # Layer normalization\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout1 = nn.Dropout(p=dropout)\n",
        "        self.dropout2 = nn.Dropout(p=dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Tensor, shape [batch_size, seq_len, d_model]\n",
        "            mask: Tensor, shape [batch_size, 1, 1, seq_len]\n",
        "        \"\"\"\n",
        "        # Multi-head attention with residual connection and layer normalization\n",
        "        attn_output, _ = self.self_attn(x, x, x, mask)\n",
        "        attn_output = self.dropout1(attn_output)\n",
        "        out1 = self.norm1(x + attn_output)\n",
        "\n",
        "        # Feed-forward with residual connection and layer normalization\n",
        "        ff_output = self.feed_forward(out1)\n",
        "        ff_output = self.dropout2(ff_output)\n",
        "        out2 = self.norm2(out1 + ff_output)\n",
        "\n",
        "        return out2\n",
        "\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Single layer of the decoder.\n",
        "    Consists of masked multi-head self-attention, multi-head attention over encoder output,\n",
        "    and position-wise feed-forward networks.\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "\n",
        "        # Multi-head attention (self-attention)\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
        "\n",
        "        # Multi-head attention (encoder-decoder attention)\n",
        "        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
        "\n",
        "        # Feed-forward network\n",
        "        self.feed_forward = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
        "\n",
        "        # Layer normalization\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout1 = nn.Dropout(p=dropout)\n",
        "        self.dropout2 = nn.Dropout(p=dropout)\n",
        "        self.dropout3 = nn.Dropout(p=dropout)\n",
        "\n",
        "    def forward(self, x, enc_output, look_ahead_mask=None, padding_mask=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Tensor, shape [batch_size, seq_len, d_model]\n",
        "            enc_output: Tensor, shape [batch_size, enc_seq_len, d_model]\n",
        "            look_ahead_mask: Tensor to mask future positions, shape [batch_size, 1, seq_len, seq_len]\n",
        "            padding_mask: Tensor to mask padding tokens, shape [batch_size, 1, 1, enc_seq_len]\n",
        "        \"\"\"\n",
        "        # Masked multi-head self-attention with residual connection and layer normalization\n",
        "        attn1_output, _ = self.self_attn(x, x, x, look_ahead_mask)\n",
        "        attn1_output = self.dropout1(attn1_output)\n",
        "        out1 = self.norm1(x + attn1_output)\n",
        "\n",
        "        # Multi-head attention over encoder output with residual connection and layer normalization\n",
        "        attn2_output, _ = self.enc_dec_attn(out1, enc_output, enc_output, padding_mask)\n",
        "        attn2_output = self.dropout2(attn2_output)\n",
        "        out2 = self.norm2(out1 + attn2_output)\n",
        "\n",
        "        # Feed-forward with residual connection and layer normalization\n",
        "        ff_output = self.feed_forward(out2)\n",
        "        ff_output = self.dropout3(ff_output)\n",
        "        out3 = self.norm3(out2 + ff_output)\n",
        "\n",
        "        return out3\n",
        "\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Full encoder stack consisting of N identical layers.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_layers, d_model, num_heads, d_ff, input_vocab_size,\n",
        "                 max_seq_length=5000, dropout=0.1):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # Embedding layer\n",
        "        self.embedding = nn.Embedding(input_vocab_size, d_model)\n",
        "\n",
        "        # Positional encoding\n",
        "        self.pos_encoding = PositionalEncoding(d_model, max_seq_length, dropout)\n",
        "\n",
        "        # Encoder layers\n",
        "        self.layers = nn.ModuleList([\n",
        "            EncoderLayer(d_model, num_heads, d_ff, dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Tensor, shape [batch_size, seq_len]\n",
        "            mask: Tensor to mask padding tokens, shape [batch_size, 1, 1, seq_len]\n",
        "        \"\"\"\n",
        "        seq_len = x.size(1)\n",
        "\n",
        "        # Embedding and scaling\n",
        "        x = self.embedding(x) * math.sqrt(self.d_model)\n",
        "\n",
        "        # Add positional encoding\n",
        "        x = self.pos_encoding(x)\n",
        "\n",
        "        # Process through encoder layers\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "\n",
        "        return x  # (batch_size, seq_len, d_model)\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Full decoder stack consisting of N identical layers.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_layers, d_model, num_heads, d_ff, target_vocab_size,\n",
        "                 max_seq_length=5000, dropout=0.1):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # Embedding layer\n",
        "        self.embedding = nn.Embedding(target_vocab_size, d_model)\n",
        "\n",
        "        # Positional encoding\n",
        "        self.pos_encoding = PositionalEncoding(d_model, max_seq_length, dropout)\n",
        "\n",
        "        # Decoder layers\n",
        "        self.layers = nn.ModuleList([\n",
        "            DecoderLayer(d_model, num_heads, d_ff, dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x, enc_output, look_ahead_mask=None, padding_mask=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Tensor, shape [batch_size, seq_len]\n",
        "            enc_output: Encoder output, shape [batch_size, enc_seq_len, d_model]\n",
        "            look_ahead_mask: Tensor to mask future positions, shape [batch_size, 1, seq_len, seq_len]\n",
        "            padding_mask: Tensor to mask padding tokens, shape [batch_size, 1, 1, enc_seq_len]\n",
        "        \"\"\"\n",
        "        seq_len = x.size(1)\n",
        "\n",
        "        # Embedding and scaling\n",
        "        x = self.embedding(x) * math.sqrt(self.d_model)\n",
        "\n",
        "        # Add positional encoding\n",
        "        x = self.pos_encoding(x)\n",
        "\n",
        "        # Process through decoder layers\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, enc_output, look_ahead_mask, padding_mask)\n",
        "\n",
        "        return x  # (batch_size, seq_len, d_model)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9AmpAbONSfKQ",
        "outputId": "dbba0400-6518-4fc0-e06b-cb4689fad330"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.0.0+cu117\n",
            "Using device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Transformer(nn.Module):\n",
        "    \"\"\"\n",
        "    Full transformer model as described in \"Attention Is All You Need\".\n",
        "    Consists of an encoder and a decoder.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_layers, d_model, num_heads, d_ff, input_vocab_size,\n",
        "                 target_vocab_size, max_seq_length=5000, dropout=0.1):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        self.encoder = Encoder(num_layers, d_model, num_heads, d_ff,\n",
        "                               input_vocab_size, max_seq_length, dropout)\n",
        "\n",
        "        self.decoder = Decoder(num_layers, d_model, num_heads, d_ff,\n",
        "                               target_vocab_size, max_seq_length, dropout)\n",
        "\n",
        "        self.final_layer = nn.Linear(d_model, target_vocab_size)\n",
        "\n",
        "    def forward(self, inp, tar, enc_padding_mask=None, look_ahead_mask=None, dec_padding_mask=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            inp: Encoder input, shape [batch_size, inp_seq_len]\n",
        "            tar: Decoder input, shape [batch_size, tar_seq_len]\n",
        "            enc_padding_mask: Mask for encoder, shape [batch_size, 1, 1, inp_seq_len]\n",
        "            look_ahead_mask: Mask for decoder self-attention, shape [batch_size, 1, tar_seq_len, tar_seq_len]\n",
        "            dec_padding_mask: Mask for decoder attention over encoder output, shape [batch_size, 1, 1, inp_seq_len]\n",
        "        \"\"\"\n",
        "        # Encoder\n",
        "        enc_output = self.encoder(inp, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
        "\n",
        "        # Decoder\n",
        "        dec_output = self.decoder(tar, enc_output, look_ahead_mask, dec_padding_mask)  # (batch_size, tar_seq_len, d_model)\n",
        "\n",
        "        # Final linear layer\n",
        "        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
        "\n",
        "        return final_output\n",
        "\n",
        "\n",
        "def create_padding_mask(seq):\n",
        "    \"\"\"\n",
        "    Creates mask for padding tokens (value 0).\n",
        "\n",
        "    Args:\n",
        "        seq: Tensor of shape [batch_size, seq_len]\n",
        "\n",
        "    Returns:\n",
        "        mask: Tensor of shape [batch_size, 1, 1, seq_len]\n",
        "    \"\"\"\n",
        "    # Create mask for padding tokens (value 0)\n",
        "    mask = (seq == 0).float()\n",
        "\n",
        "    # Add dimensions for multi-head attention\n",
        "    return mask.unsqueeze(1).unsqueeze(2)  # (batch_size, 1, 1, seq_len)\n",
        "\n",
        "\n",
        "def create_look_ahead_mask(size):\n",
        "    \"\"\"\n",
        "    Creates mask to prevent attention to future tokens.\n",
        "\n",
        "    Args:\n",
        "        size: Integer, size of the mask\n",
        "\n",
        "    Returns:\n",
        "        mask: Tensor of shape [size, size] with 1s in the lower triangular part\n",
        "    \"\"\"\n",
        "    # Create a lower triangular matrix with 1s and 0s on the upper triangular\n",
        "    mask = torch.triu(torch.ones(size, size), diagonal=1).float()\n",
        "\n",
        "    # Invert the mask, setting 1s to 0s and 0s to 1s\n",
        "    return 1 - mask\n",
        "\n",
        "\n",
        "def create_masks(inp, tar):\n",
        "    \"\"\"\n",
        "    Creates all the masks needed for transformer training.\n",
        "\n",
        "    Args:\n",
        "        inp: Input tensor, shape [batch_size, inp_seq_len]\n",
        "        tar: Target tensor, shape [batch_size, tar_seq_len]\n",
        "\n",
        "    Returns:\n",
        "        enc_padding_mask: Mask for encoder padding\n",
        "        combined_mask: Combined look ahead and padding mask for decoder\n",
        "        dec_padding_mask: Padding mask for decoder\n",
        "    \"\"\"\n",
        "    # Encoder padding mask\n",
        "    enc_padding_mask = create_padding_mask(inp)\n",
        "\n",
        "    # Decoder padding mask (for attention over encoder output)\n",
        "    dec_padding_mask = create_padding_mask(inp)\n",
        "\n",
        "    # Look ahead mask for decoder self-attention\n",
        "    look_ahead_mask = create_look_ahead_mask(tar.size(1))\n",
        "    dec_target_padding_mask = create_padding_mask(tar)\n",
        "\n",
        "    # Combine the masks (using broadcasting)\n",
        "    combined_mask = torch.max(\n",
        "        dec_target_padding_mask,\n",
        "        look_ahead_mask.unsqueeze(0).to(tar.device)\n",
        "    )\n",
        "\n",
        "    return enc_padding_mask, combined_mask, dec_padding_mask\n"
      ],
      "metadata": {
        "id": "jrlWvb9pNPoA"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class WarmupThenDecaySchedule:\n",
        "    \"\"\"\n",
        "    Learning rate schedule with warmup and decay as described in the paper.\n",
        "    \"\"\"\n",
        "    def __init__(self, optimizer, d_model, warmup_steps=4000):\n",
        "        self.optimizer = optimizer\n",
        "        self.d_model = d_model\n",
        "        self.warmup_steps = warmup_steps\n",
        "        self.current_step = 0\n",
        "\n",
        "    def step(self):\n",
        "        \"\"\"\n",
        "        Update learning rate and take an optimization step.\n",
        "        \"\"\"\n",
        "        self.current_step += 1\n",
        "        lr = self._get_lr()\n",
        "        for param_group in self.optimizer.param_groups:\n",
        "            param_group['lr'] = lr\n",
        "        self.optimizer.step()\n",
        "\n",
        "    def _get_lr(self):\n",
        "        \"\"\"\n",
        "        Computes learning rate based on the formula in the paper.\n",
        "        \"\"\"\n",
        "        arg1 = self.current_step ** -0.5\n",
        "        arg2 = self.current_step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "        return (self.d_model ** -0.5) * min(arg1, arg2)\n",
        "\n",
        "    def zero_grad(self):\n",
        "        \"\"\"\n",
        "        Clear the gradients of all optimized parameters.\n",
        "        \"\"\"\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "\n",
        "def train_transformer(model, train_dataloader, criterion, optimizer_schedule, n_epochs, device):\n",
        "    \"\"\"\n",
        "    Train the transformer model.\n",
        "\n",
        "    Args:\n",
        "        model: Transformer model\n",
        "        train_dataloader: DataLoader for training data\n",
        "        criterion: Loss function\n",
        "        optimizer_schedule: Learning rate scheduler\n",
        "        n_epochs: Number of epochs\n",
        "        device: Device to train on\n",
        "\n",
        "    Returns:\n",
        "        losses: List of training losses\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    losses = []\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        total_loss = 0\n",
        "\n",
        "        for batch_idx, (inp, tar) in enumerate(train_dataloader):\n",
        "            inp, tar = inp.to(device), tar.to(device)\n",
        "\n",
        "            # Create target input and output\n",
        "            tar_inp = tar[:, :-1]  # remove last token\n",
        "            tar_real = tar[:, 1:]  # remove first token (start token)\n",
        "\n",
        "            # Create masks\n",
        "            enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
        "\n",
        "            # Forward pass\n",
        "            optimizer_schedule.zero_grad()\n",
        "\n",
        "            outputs = model(\n",
        "                inp,\n",
        "                tar_inp,\n",
        "                enc_padding_mask,\n",
        "                combined_mask,\n",
        "                dec_padding_mask\n",
        "            )\n",
        "\n",
        "            # Calculate loss\n",
        "            loss = criterion(\n",
        "                outputs.contiguous().view(-1, outputs.size(-1)),\n",
        "                tar_real.contiguous().view(-1)\n",
        "            )\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "\n",
        "            # Update parameters\n",
        "            optimizer_schedule.step()\n",
        "\n",
        "            # Track loss\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Print progress\n",
        "            if (batch_idx + 1) % 10 == 0:\n",
        "                print(f'Epoch {epoch+1}/{n_epochs}, Batch {batch_idx+1}/{len(train_dataloader)}, Loss: {loss.item():.4f}')\n",
        "\n",
        "        # Calculate average loss\n",
        "        avg_loss = total_loss / len(train_dataloader)\n",
        "        losses.append(avg_loss)\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{n_epochs}, Average Loss: {avg_loss:.4f}')\n",
        "\n",
        "    return losses\n",
        "\n",
        "\n",
        "\n",
        "def greedy_decode(model, src, max_len, start_symbol, device):\n",
        "    \"\"\"\n",
        "    Perform greedy decoding.\n",
        "\n",
        "    Args:\n",
        "        model: Trained transformer model\n",
        "        src: Source sequence, shape [1, src_seq_len]\n",
        "        max_len: Maximum length of the output sequence\n",
        "        start_symbol: Start token id\n",
        "        device: Device to run inference on\n",
        "\n",
        "    Returns:\n",
        "        decoded_sequence: Decoded sequence\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Create encoder input\n",
        "    src = src.to(device)\n",
        "\n",
        "    # Encoder output\n",
        "    enc_padding_mask = create_padding_mask(src)\n",
        "    enc_output = model.encoder(src, enc_padding_mask)\n",
        "\n",
        "    # Initialize decoder input with start token\n",
        "    dec_input = torch.zeros(1, 1).fill_(start_symbol).type_as(src).to(device)\n",
        "\n",
        "    # Decode one token at a time\n",
        "    for i in range(max_len - 1):\n",
        "        # Create masks\n",
        "        dec_padding_mask = create_padding_mask(src)\n",
        "        look_ahead_mask = create_look_ahead_mask(dec_input.size(1)).to(device)\n",
        "        combined_mask = torch.max(\n",
        "            create_padding_mask(dec_input),\n",
        "            look_ahead_mask.unsqueeze(0)\n",
        "        )\n",
        "\n",
        "        # Decoder output\n",
        "        dec_output = model.decoder(\n",
        "            dec_input,\n",
        "            enc_output,\n",
        "            combined_mask,\n",
        "            dec_padding_mask\n",
        "        )\n",
        "\n",
        "        # Final output and probabilities\n",
        "        output = model.final_layer(dec_output)  # (1, seq_len, vocab_size)\n",
        "        output = output[:, -1, :]  # (1, vocab_size)\n",
        "\n",
        "        # Greedy decoding - pick the most probable token\n",
        "        _, predicted = torch.max(output, dim=1)\n",
        "\n",
        "        # Concatenate predicted token to decoder input\n",
        "        dec_input = torch.cat(\n",
        "            [dec_input, predicted.unsqueeze(1)],\n",
        "            dim=1\n",
        "        )\n",
        "\n",
        "        # Break if end token is predicted\n",
        "        if predicted.item() == 0:  # Assuming 0 is end token or padding\n",
        "            break\n",
        "\n",
        "    return dec_input.squeeze().cpu().numpy()\n",
        "\n",
        "\n",
        "\n",
        "class SyntheticDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Simple synthetic dataset for sequence-to-sequence tasks.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_samples=1000, src_seq_len=10, tgt_seq_len=10,\n",
        "                 src_vocab_size=100, tgt_vocab_size=100):\n",
        "        self.num_samples = num_samples\n",
        "\n",
        "        # Generate random source sequences\n",
        "        self.src_data = torch.randint(\n",
        "            1, src_vocab_size, (num_samples, src_seq_len)\n",
        "        )\n",
        "\n",
        "        # For simplicity, target is just the source sequence with digits reversed\n",
        "        self.tgt_data = torch.flip(self.src_data, dims=[1])\n",
        "\n",
        "        # Add start token (assuming tgt_vocab_size-1 is the start token)\n",
        "        start_tokens = torch.full((num_samples, 1), tgt_vocab_size-1)\n",
        "        self.tgt_data = torch.cat([start_tokens, self.tgt_data], dim=1)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_samples\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.src_data[idx], self.tgt_data[idx]\n",
        "\n",
        "\n",
        "# --- Model Hyperparameters ---\n",
        "num_layers = 4\n",
        "d_model = 128\n",
        "num_heads = 8\n",
        "d_ff = 512\n",
        "input_vocab_size = 100\n",
        "target_vocab_size = 100\n",
        "dropout_rate = 0.1\n",
        "batch_size = 32\n",
        "num_epochs = 10\n",
        "\n",
        "# --- Create Synthetic Dataset ---\n",
        "train_dataset = SyntheticDataset(\n",
        "    num_samples=1000,\n",
        "    src_seq_len=10,\n",
        "    tgt_seq_len=11,  # +1 for start token\n",
        "    src_vocab_size=input_vocab_size,\n",
        "    tgt_vocab_size=target_vocab_size\n",
        ")\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "# --- Initialize Model ---\n",
        "transformer = Transformer(\n",
        "    num_layers=num_layers,\n",
        "    d_model=d_model,\n",
        "    num_heads=num_heads,\n",
        "    d_ff=d_ff,\n",
        "    input_vocab_size=input_vocab_size,\n",
        "    target_vocab_size=target_vocab_size,\n",
        "    dropout=dropout_rate\n",
        ").to(device)\n",
        "\n",
        "# --- Loss Function and Optimizer ---\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)  # Ignore padding token (0)\n",
        "optimizer = optim.Adam(\n",
        "    transformer.parameters(),\n",
        "    lr=0,\n",
        "    betas=(0.9, 0.98),\n",
        "    eps=1e-9\n",
        ")\n",
        "\n",
        "# --- Learning Rate Schedule ---\n",
        "lr_schedule = WarmupThenDecaySchedule(\n",
        "    optimizer,\n",
        "    d_model,\n",
        "    warmup_steps=4000\n",
        ")\n",
        "\n",
        "# --- Train the Model ---\n",
        "losses = train_transformer(\n",
        "    transformer,\n",
        "    train_dataloader,\n",
        "    criterion,\n",
        "    lr_schedule,\n",
        "    num_epochs,\n",
        "    device\n",
        ")\n",
        "\n",
        "# --- Plot Training Loss ---\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(losses)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# --- Test Inference ---\n",
        "def test_inference():\n",
        "    # Select a random sample from the training set\n",
        "    idx = np.random.randint(0, len(train_dataset))\n",
        "    src, tgt = train_dataset[idx]\n",
        "\n",
        "    # Prepare source tensor\n",
        "    src = src.unsqueeze(0).to(device)  # Add batch dimension\n",
        "\n",
        "    # Inference\n",
        "    start_symbol = target_vocab_size - 1  # Start token\n",
        "    output = greedy_decode(transformer, src, max_len=20, start_symbol=start_symbol, device=device)\n",
        "\n",
        "    # Print results\n",
        "    print(f\"Source sequence: {src.squeeze().cpu().numpy()}\")\n",
        "    print(f\"Target sequence: {tgt.numpy()}\")\n",
        "    print(f\"Predicted sequence: {output}\")\n",
        "\n",
        "    # Check accuracy\n",
        "    target_seq = tgt[1:].numpy()  # Remove start token\n",
        "    pred_seq = output[1:]         # Remove start token\n",
        "\n",
        "    # Truncate sequences to compare\n",
        "    min_len = min(len(target_seq), len(pred_seq))\n",
        "    target_seq = target_seq[:min_len]\n",
        "    pred_seq = pred_seq[:min_len]\n",
        "\n",
        "    accuracy = np.mean(target_seq == pred_seq) * 100\n",
        "    print(f\"Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "test_inference()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CZvFZ2MKY_on",
        "outputId": "bacf14e4-53f5-4981-a6b6-1e68e08fb74d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Batch 10/32, Loss: 4.7749\n",
            "Epoch 1/10, Batch 20/32, Loss: 4.7573\n",
            "Epoch 1/10, Batch 30/32, Loss: 4.7760\n",
            "Epoch 1/10, Average Loss: 4.7741\n",
            "Epoch 2/10, Batch 10/32, Loss: 4.7618\n",
            "Epoch 2/10, Batch 20/32, Loss: 4.7221\n",
            "Epoch 2/10, Batch 30/32, Loss: 4.7017\n",
            "Epoch 2/10, Average Loss: 4.7355\n",
            "Epoch 3/10, Batch 10/32, Loss: 4.6829\n",
            "Epoch 3/10, Batch 20/32, Loss: 4.6194\n",
            "Epoch 3/10, Batch 30/32, Loss: 4.6635\n",
            "Epoch 3/10, Average Loss: 4.6791\n",
            "Epoch 4/10, Batch 10/32, Loss: 4.6393\n",
            "Epoch 4/10, Batch 20/32, Loss: 4.5930\n",
            "Epoch 4/10, Batch 30/32, Loss: 4.6068\n",
            "Epoch 4/10, Average Loss: 4.6221\n",
            "Epoch 5/10, Batch 10/32, Loss: 4.5653\n",
            "Epoch 5/10, Batch 20/32, Loss: 4.5630\n",
            "Epoch 5/10, Batch 30/32, Loss: 4.5717\n",
            "Epoch 5/10, Average Loss: 4.5779\n",
            "Epoch 6/10, Batch 10/32, Loss: 4.5128\n",
            "Epoch 6/10, Batch 20/32, Loss: 4.5313\n",
            "Epoch 6/10, Batch 30/32, Loss: 4.4995\n",
            "Epoch 6/10, Average Loss: 4.5199\n",
            "Epoch 7/10, Batch 10/32, Loss: 4.4308\n",
            "Epoch 7/10, Batch 20/32, Loss: 4.4241\n",
            "Epoch 7/10, Batch 30/32, Loss: 4.3776\n",
            "Epoch 7/10, Average Loss: 4.4387\n",
            "Epoch 8/10, Batch 10/32, Loss: 4.3313\n",
            "Epoch 8/10, Batch 20/32, Loss: 4.2678\n",
            "Epoch 8/10, Batch 30/32, Loss: 4.2758\n",
            "Epoch 8/10, Average Loss: 4.3057\n",
            "Epoch 9/10, Batch 10/32, Loss: 4.2064\n",
            "Epoch 9/10, Batch 20/32, Loss: 4.1362\n",
            "Epoch 9/10, Batch 30/32, Loss: 4.1712\n",
            "Epoch 9/10, Average Loss: 4.1649\n",
            "Epoch 10/10, Batch 10/32, Loss: 4.1346\n",
            "Epoch 10/10, Batch 20/32, Loss: 4.0897\n",
            "Epoch 10/10, Batch 30/32, Loss: 4.0492\n",
            "Epoch 10/10, Average Loss: 4.0258\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Source sequence: [93 44 12 54 61 64 71 16 61 53]\n",
            "Target sequence: [99 53 61 16 71 64 61 54 12 44 93]\n",
            "Predicted sequence: [99 12 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61 61]\n",
            "Accuracy: 20.00%\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}